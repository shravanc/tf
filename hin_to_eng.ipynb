{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hin-to-eng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMezl9puNjQPBDev1VJD2l8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravanc/tf/blob/master/hin_to_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_c9ADMlRamA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "bba4d34e-e28a-4c3b-9569-193d29b9778b"
      },
      "source": [
        "#==================================Downloading===========================================\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "http = urllib3.PoolManager()\n",
        "\n",
        "def extract(path, url, zipfilename):\n",
        "  with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "  print(\"file--->\", zipfilename)\n",
        "  with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)\n",
        "\n",
        "\n",
        "# Dataset 1\n",
        "url = 'https://github.com/shravanc/datasets/blob/master/hin_eng/hi-en.zip?raw=true'\n",
        "filename = 'hi-en.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "extract(path, url, zipfilename)\n",
        "\n",
        "\n",
        "# Dataset 2\n",
        "url = 'https://github.com/shravanc/datasets/blob/master/hin_eng/parallel_corpora.zip?raw=true'\n",
        "filename = 'parallel_corpora.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "extract(path, url, zipfilename)\n",
        "\n",
        "#==================================Downloading==========================================="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file---> /content/hi-en.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file---> /content/parallel_corpora.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I__mDa3Rt0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7630580-e5aa-4045-bd82-3ac2043bfd96"
      },
      "source": [
        "!ls hi-en/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.en\tdev.hi\tREADME\ttest.en  test.hi  train.en  train.hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9nwpYL7Wyl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06176ba7-8fef-48fe-bbe7-d8670c8abfa9"
      },
      "source": [
        "!ls parallel_corpora/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt  hin.txt  README\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C_F8MSpXHzP",
        "colab_type": "text"
      },
      "source": [
        "**Feel free to check the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PJGFEPXW5LY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1c1246f-cec7-44ce-c17a-697362380a83"
      },
      "source": [
        "!git clone https://github.com/shravanc/tf.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'tf' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejl1JLyXXhKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e3eeacc-f6f1-4e6d-e53d-d72f2720a089"
      },
      "source": [
        "!ls tf/hi_to_en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "colab_lib  lib\tmain.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSdvbXvFXlYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf.hi_to_en.colab_lib.utils import unicode_to_ascii, preprocess_sentence, create_dataset, create_new_dataset, load_dataset, max_length, convert\n",
        "from tf.hi_to_en.colab_lib.encoder import Encoder\n",
        "from tf.hi_to_en.colab_lib.attention import BahdanauAttention\n",
        "from tf.hi_to_en.colab_lib.decoder import Decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM52obphbVXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e647540a-5b7d-4b2f-bb57-5e7634714285"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "tf.executing_eagerly()\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8h2c_dQd52W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ae813e59-f6f6-4f07-8874-44878bbbd9b3"
      },
      "source": [
        "#==================================DataPreparation=======================================\n",
        "# from lib.utils import unicode_to_ascii, preprocess_sentence, create_dataset, create_new_dataset, load_dataset, max_length, convert\n",
        "\n",
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"क्या मैं यह पुस्तक उधार ले सकता हूँ?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))\n",
        "\n",
        "path_to_file = os.path.join(os.getcwd(), \"parallel_corpora/hin.txt\")\n",
        "en_1, hi_1 = create_dataset(path_to_file, None)\n",
        "\n",
        "en_path = os.path.join(os.getcwd(), \"hi-en/train.en\")\n",
        "hi_path = os.path.join(os.getcwd(), \"hi-en/train.hi\")\n",
        "en_2, hi_2 = create_new_dataset(en_path, hi_path)\n",
        "\n",
        "en = en_1 + en_2\n",
        "hi = hi_1 + hi_2\n",
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 80000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])\n",
        "\n",
        "\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "#==================================DataPreparation=======================================\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xe0\\xa4\\x95\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xae \\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\xaa\\xe0\\xa4\\xb8\\xe0\\xa4\\xa4\\xe0\\xa4\\x95 \\xe0\\xa4\\x89\\xe0\\xa4\\xa7\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0 \\xe0\\xa4\\xb2 \\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9 ? <end>'\n",
            "2223 2223 556 556\n",
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> वह\n",
            "1534 ----> जलद\n",
            "46 ----> ही\n",
            "166 ----> ठीक\n",
            "15 ----> हो\n",
            "245 ----> जाएगा।\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "1328 ----> he'll\n",
            "71 ----> get\n",
            "137 ----> well\n",
            "189 ----> soon\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 29]), TensorShape([64, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY0fTkjRe6iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "925410e6-46dd-45de-f1c5-f2c8a7b9e69f"
      },
      "source": [
        "#==================================Encoder===============================================\n",
        "# from lib.encoder import Encoder\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "\n",
        "#==================================Encoder==============================================="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 29, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMZjNUkogmJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "86c92610-2e7d-4b25-c6c7-f91389956b0d"
      },
      "source": [
        "#==================================Attention=============================================\n",
        "# from lib.attention import BahdanauAttention\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
        "\n",
        "#==================================Attention============================================="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 29, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEe3L-RhMpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5dfdeac-01ff-4c8a-95a3-7c43a78b4cec"
      },
      "source": [
        "#==================================Dencoder==============================================\n",
        "# from lib.decoder import Decoder\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "\n",
        "#==================================Dencoder==============================================\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 2389)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOpEfQ4zhSVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==================================Optimizer=============================================\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "#==================================Optimizer============================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7bYnIL6hVIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==================================CheckPoint============================================\n",
        "checkpoint_dir = './training_checkpoints_1'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "#==================================CheckPoint============================================\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKlv4haOhXGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==================================Training==============================================\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss\n",
        "#==================================Training=============================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzIOuiaShZyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5cc49e3-549f-4633-d014-49c498743daf"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.3413\n",
            "Epoch 1 Loss 0.3819\n",
            "Time taken for 1 epoch 23.20542573928833 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3277\n",
            "Epoch 2 Loss 0.3394\n",
            "Time taken for 1 epoch 7.379666805267334 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2700\n",
            "Epoch 3 Loss 0.2991\n",
            "Time taken for 1 epoch 7.037050008773804 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2328\n",
            "Epoch 4 Loss 0.2568\n",
            "Time taken for 1 epoch 7.561476230621338 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1660\n",
            "Epoch 5 Loss 0.2189\n",
            "Time taken for 1 epoch 7.099998712539673 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1560\n",
            "Epoch 6 Loss 0.1807\n",
            "Time taken for 1 epoch 7.611353397369385 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1122\n",
            "Epoch 7 Loss 0.1505\n",
            "Time taken for 1 epoch 7.187697172164917 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1084\n",
            "Epoch 8 Loss 0.1265\n",
            "Time taken for 1 epoch 7.62800407409668 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1022\n",
            "Epoch 9 Loss 0.1024\n",
            "Time taken for 1 epoch 7.2221949100494385 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0724\n",
            "Epoch 10 Loss 0.0845\n",
            "Time taken for 1 epoch 7.702613353729248 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0718\n",
            "Epoch 11 Loss 0.0670\n",
            "Time taken for 1 epoch 7.278635025024414 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0418\n",
            "Epoch 12 Loss 0.0524\n",
            "Time taken for 1 epoch 7.804001331329346 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0368\n",
            "Epoch 13 Loss 0.0402\n",
            "Time taken for 1 epoch 7.2570061683654785 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0287\n",
            "Epoch 14 Loss 0.0311\n",
            "Time taken for 1 epoch 7.710827827453613 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0249\n",
            "Epoch 15 Loss 0.0237\n",
            "Time taken for 1 epoch 7.2494590282440186 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0148\n",
            "Epoch 16 Loss 0.0198\n",
            "Time taken for 1 epoch 7.661465883255005 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0159\n",
            "Epoch 17 Loss 0.0167\n",
            "Time taken for 1 epoch 7.226186037063599 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0174\n",
            "Epoch 18 Loss 0.0148\n",
            "Time taken for 1 epoch 7.668620824813843 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0089\n",
            "Epoch 19 Loss 0.0133\n",
            "Time taken for 1 epoch 7.219729900360107 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0124\n",
            "Epoch 20 Loss 0.0121\n",
            "Time taken for 1 epoch 7.6785149574279785 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0102\n",
            "Epoch 21 Loss 0.0107\n",
            "Time taken for 1 epoch 7.221122980117798 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0064\n",
            "Epoch 22 Loss 0.0098\n",
            "Time taken for 1 epoch 7.679462194442749 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0092\n",
            "Epoch 23 Loss 0.0097\n",
            "Time taken for 1 epoch 7.262720823287964 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0058\n",
            "Epoch 24 Loss 0.0083\n",
            "Time taken for 1 epoch 7.666788578033447 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0051\n",
            "Epoch 25 Loss 0.0083\n",
            "Time taken for 1 epoch 7.238133907318115 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0127\n",
            "Epoch 26 Loss 0.0080\n",
            "Time taken for 1 epoch 7.741947650909424 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0049\n",
            "Epoch 27 Loss 0.0075\n",
            "Time taken for 1 epoch 7.258825302124023 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0137\n",
            "Epoch 28 Loss 0.0076\n",
            "Time taken for 1 epoch 7.743351221084595 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0125\n",
            "Epoch 29 Loss 0.0112\n",
            "Time taken for 1 epoch 7.249551057815552 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0125\n",
            "Epoch 30 Loss 0.0224\n",
            "Time taken for 1 epoch 7.684588193893433 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-MhnNo0hc4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==================================Evaluate===========================================\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  #inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = []\n",
        "  for i in sentence.split(' '):\n",
        "    if i == \"\":\n",
        "      break\n",
        "    wi = inp_lang.word_index.get(i)\n",
        "    print(\"index--->\", i)\n",
        "    print(\"class--->\", type(inp_lang.word_index.get(i)))\n",
        "    if wi:\n",
        "      inputs.append(wi)\n",
        "\n",
        "\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot\n",
        "#==================================Evaluate===========================================\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3RE0rN0iVl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thkjozVyiY0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43afeae8-d8f9-4d64-e399-8af1773d4925"
      },
      "source": [
        "#==================================RestoreCheckPoint===================================\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "#==================================RestoreCheckPoint==================================="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fdcb9569080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xLyfzQvibU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9c524be0-e7fb-41c5-d83d-5b5be5f56fbc"
      },
      "source": [
        "translate(u'देखने के लिए धन्यवाद')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index---> <start>\n",
            "class---> <class 'int'>\n",
            "index---> दखन\n",
            "class---> <class 'int'>\n",
            "index---> क\n",
            "class---> <class 'int'>\n",
            "index---> लिए\n",
            "class---> <class 'int'>\n",
            "index---> धनयवाद\n",
            "class---> <class 'NoneType'>\n",
            "index---> <end>\n",
            "class---> <class 'int'>\n",
            "Input: <start> दखन क लिए धनयवाद <end>\n",
            "Predicted translation: you're a big like . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYMnsXWnieA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}